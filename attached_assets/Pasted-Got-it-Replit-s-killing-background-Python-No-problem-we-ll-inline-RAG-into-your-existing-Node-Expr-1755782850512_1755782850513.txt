Got it. Replit’s killing background Python? No problem—we’ll inline RAG into your existing Node/Express server and keep rolling. Zero sidecar. One code path. Let’s ship.

Below is a drop‑in Express router that:
	•	ingests ESPN + NFL.com (RSS, with HTML fallback),
	•	stores news in SQLite,
	•	does BM25 + recency boost retrieval (fast, no ML server),
	•	generates short strong takes (deterministic for now),
	•	exposes the same endpoints you already planned:
POST /admin/rag/ingest, GET /api/rag/players/search, GET /api/rag/take.

⸻

1) Install deps

npm i express better-sqlite3 node-fetch@3 rss-parser cheerio wink-bm25-text-search

If your project isn’t ESM yet, add: npm i node-fetch-commonjs and replace imports accordingly (notes inline).

⸻

2) Create rag.router.js

// rag.router.js  — RAG-for-OTC (Node inline)  | v0.3
// Plug this router into your existing Express app.
// Env (optional):
//   RAG_DB=rag_news.db
//   NEWS_MAX_AGE_DAYS=21
//   RAG_TOPK=5
//   RAG_MIN_SOURCES=2

import express from "express";
import Database from "better-sqlite3";
import fetch from "node-fetch";                 // if CJS: const fetch = (...)=>(import('node-fetch-commonjs').then(m=>m.default(...)));
import RSSParser from "rss-parser";
import cheerio from "cheerio";
import bm25Factory from "wink-bm25-text-search";

const ESPN_RSS = process.env.ESPN_RSS || "https://www.espn.com/espn/rss/nfl/news";
const NFL_RSS  = process.env.NFL_RSS  || "https://www.nfl.com/rss/rsslanding?searchString=news";
const NFL_NEWS_FALLBACK = process.env.NFL_NEWS_FALLBACK || "https://www.nfl.com/news/";
const DB_PATH  = process.env.RAG_DB || "rag_news.db";

const NEWS_MAX_AGE_DAYS = Number(process.env.NEWS_MAX_AGE_DAYS || 21);
const RAG_TOPK = Number(process.env.RAG_TOPK || 5);
const RAG_MIN_SOURCES = Number(process.env.RAG_MIN_SOURCES || 2);

// ----- tiny utils
const norm = (s="") => s.toLowerCase().replace(/[^a-z]/g, "");
const sha = (s) => cryptoHash(s).slice(0, 32);
function cryptoHash(s) { // quick non-crypto sha256 fallback using Node crypto
  const { createHash } = await import("crypto"); // top-level import not available in CJS; change if needed
  return createHash("sha256").update(s).digest("hex");
}
function parseIsoOrNow(iso) {
  const d = new Date(iso || Date.now());
  return isNaN(d.getTime()) ? new Date() : d;
}
function withinAge(iso, maxDays=NEWS_MAX_AGE_DAYS) {
  const d = parseIsoOrNow(iso);
  const days = (Date.now() - d.getTime()) / (1000*60*60*24);
  return days <= maxDays;
}
function recencyBoost(iso) {
  const d = parseIsoOrNow(iso);
  const days = Math.max(0, (Date.now() - d.getTime()) / (1000*60*60*24));
  return 1 / (1 + (days/7)); // ~1 fresh, ~0.5 @ 7d, ~0.25 @ 21d
}

// ----- DB
const db = new Database(DB_PATH);
db.pragma("journal_mode = WAL");
db.exec(`
  CREATE TABLE IF NOT EXISTS articles (
    id TEXT PRIMARY KEY,
    source TEXT,
    title TEXT,
    url TEXT,
    published_at TEXT,
    text TEXT,
    team_tags TEXT,
    player_ids TEXT
  );
  CREATE TABLE IF NOT EXISTS players (
    player_id TEXT PRIMARY KEY,
    name TEXT,
    team TEXT,
    position TEXT,
    name_key TEXT
  );
`);

// ----- BM25 index (in-memory; rebuilt on ingest)
const bm25 = bm25Factory();
let _docs = new Map(); // id -> {title,text,published_at,source}

function buildIndex() {
  bm25.clear();
  _docs.clear();
  const rows = db.prepare("SELECT id, title, text, published_at, source FROM articles").all();
  rows.forEach((r) => {
    const text = `${r.title ?? ""} ${r.text ?? ""}`;
    bm25.addDoc({ id: r.id, body: text });
    _docs.set(r.id, r);
  });
  bm25.consolidate();
}

// ----- Sleeper dictionary sync (from your existing players table/service)
// If you already have a players table populated elsewhere, you can skip this
// and just ensure the SELECT below points to your canonical table.
async function loadPlayersFromSleeper() {
  // If you already synced players into your own DB, comment the fetch and just copy them into this table, or JOIN.
  const url = "https://api.sleeper.app/v1/players/nfl";
  const res = await fetch(url, { timeout: 60000 });
  if (!res.ok) throw new Error(`Sleeper ${res.status}`);
  const data = await res.json();

  const up = db.prepare(`
    INSERT INTO players (player_id, name, team, position, name_key)
    VALUES (@player_id, @name, @team, @position, @name_key)
    ON CONFLICT(player_id) DO UPDATE SET
      name=excluded.name, team=excluded.team, position=excluded.position, name_key=excluded.name_key
  `);

  const tx = db.transaction((arr) => arr.forEach((p) => up.run(p)));
  const rows = [];
  for (const [pid, p] of Object.entries(data)) {
    if (!p || typeof p !== "object") continue;
    const name = p.full_name || `${p.first_name||""} ${p.last_name||""}`.trim();
    if (!name || !p.position) continue;
    rows.push({
      player_id: pid,
      name,
      team: p.team || null,
      position: p.position,
      name_key: norm(name),
    });
  }
  tx(rows);
}

function nameToPlayerIds(text) {
  const keys = db.prepare("SELECT player_id, name_key FROM players").all();
  const nt = norm(text || "");
  const hits = [];
  for (const { player_id, name_key } of keys) {
    if (!name_key) continue;
    if (nt.includes(name_key)) hits.push(player_id);
  }
  return Array.from(new Set(hits));
}

// ----- Ingest
const parser = new RSSParser();

async function fetchEspn() {
  const feed = await parser.parseURL(ESPN_RSS);
  return (feed.items || []).map((e) => ({
    source: "ESPN",
    title: (e.title || "").trim(),
    url: (e.link || "").trim(),
    published_at: e.isoDate || e.pubDate || new Date().toISOString(),
    text: (e.contentSnippet || e.content || "").toString().trim(),
  })).filter(x => x.title && x.url);
}

async function fetchNflRss() {
  const feed = await parser.parseURL(NFL_RSS);
  return (feed.items || []).map((e) => ({
    source: "NFL",
    title: (e.title || "").trim(),
    url: (e.link || "").trim(),
    published_at: e.isoDate || e.pubDate || new Date().toISOString(),
    text: (e.contentSnippet || e.content || "").toString().trim(),
  })).filter(x => x.title && x.url);
}

async function fetchNflFallback() {
  try {
    const html = await (await fetch(NFL_NEWS_FALLBACK, { timeout: 30000 })).text();
    const $ = cheerio.load(html);
    const items = [];
    $("a[href*='/news/']").each((_, a) => {
      const title = $(a).text().trim();
      let url = $(a).attr("href");
      if (!title || !url) return;
      if (!/^https?:\/\//i.test(url)) url = `https://www.nfl.com${url}`;
      items.push({
        source: "NFL",
        title,
        url,
        published_at: new Date().toISOString(),
        text: title,
      });
    });
    return items;
  } catch (e) {
    console.warn("NFL fallback failed:", e.message);
    return [];
  }
}

async function fetchArticleBody(url) {
  try {
    const res = await fetch(url, { timeout: 20000 });
    const html = await res.text();
    const $ = cheerio.load(html);
    const paras = $("p").map((_, p) => $(p).text().trim()).get();
    return paras.slice(0, 12).join(" ");
  } catch {
    return "";
  }
}

async function upsertArticles(items) {
  const ins = db.prepare(`
    INSERT INTO articles (id, source, title, url, published_at, text, team_tags, player_ids)
    VALUES (@id, @source, @title, @url, @published_at, @text, @team_tags, @player_ids)
    ON CONFLICT(id) DO UPDATE SET text=excluded.text, team_tags=excluded.team_tags, player_ids=excluded.player_ids
  `);
  const tx = db.transaction((rows) => rows.forEach((r) => ins.run(r)));

  const rows = [];
  for (const it of items) {
    if (!withinAge(it.published_at)) continue;
    let body = it.text || "";
    if (body.length < 280) body = await fetchArticleBody(it.url);
    const pids = nameToPlayerIds(`${it.title} ${body}`);
    const id = await cryptoHash(`${it.source}|${it.title}|${it.url}|${it.published_at}`);
    rows.push({
      id,
      source: it.source,
      title: it.title,
      url: it.url,
      published_at: it.published_at,
      text: body,
      team_tags: JSON.stringify([]),
      player_ids: JSON.stringify(pids),
    });
  }
  tx(rows);
}

async function ingestAll() {
  try { await loadPlayersFromSleeper(); } catch (e) { console.warn("Sleeper load failed:", e.message); }
  const espn = await fetchEspn();
  let nfl = await fetchNflRss();
  if (!nfl?.length) nfl = await fetchNflFallback();
  await upsertArticles([...(espn||[]), ...(nfl||[])]);
  buildIndex();
}

// ----- Retrieval + generation
function retrieveArticles({ playerId, topic }) {
  // candidate set: any article that mentions playerId, plus bm25 on topic
  const candMap = new Map();

  if (playerId) {
    const rows = db.prepare("SELECT * FROM articles").all();
    for (const r of rows) {
      try {
        const ids = JSON.parse(r.player_ids || "[]");
        if (ids.includes(playerId)) candMap.set(r.id, r);
      } catch {}
    }
  }

  if (topic && topic.trim()) {
    const scored = bm25.search(topic).slice(0, 50);
    for (const s of scored) {
      const r = _docs.get(s[0]); // id
      if (r) candMap.set(r.id, db.prepare("SELECT * FROM articles WHERE id=?").get(r.id));
    }
  }

  // score with (bm25 if topic) + recency
  const out = [];
  const topicVec = topic && topic.trim() ? bm25.search(topic) : [];
  const tScore = new Map(topicVec); // id -> score

  for (const r of candMap.values()) {
    const base = topic ? (tScore.get(r.id) || 0) : 0;
    const fresh = recencyBoost(r.published_at || "");
    const score = 0.6 * base + 0.4 * fresh;
    out.push({ score, r });
  }

  out.sort((a, b) => b.score - a.score);
  return out.slice(0, RAG_TOPK).map(x => x.r);
}

function synthTake(player, topic, rows) {
  const name = player?.name || "Unknown";
  const team = player?.team || "";
  const pos  = player?.position || "";
  if (!rows?.length) {
    const txt = `${name} (${team}, ${pos}) has no credible updates in the last ${NEWS_MAX_AGE_DAYS} days. Default to price vs role.`;
    return {
      headline: `${name}: no fresh news; stay rational`,
      take: `• ${txt}\n• Why fade: if the market chases camp fluff.\n• Confidence: 55/100`,
      verdict: "HOLD",
      confidence: 55,
      facts: { team, position: pos, topic, articles_considered: 0 },
      citations: []
    };
  }
  const latest = rows[0];
  const dt = (latest.published_at || "").slice(0, 10);
  const sources = [...new Set(rows.map(x => x.source).filter(Boolean))].join(", ");
  let verdict = "HOLD";
  const blob = `${latest.title || ""} ${latest.text || ""}`.toLowerCase();

  if (/\b(starting|starter|elevated|increased snaps|role expands|returns from|activated)\b/.test(blob)) verdict = "BUY";
  if (/\b(injury|torn|out for|placed on ir|suspended|arrest|demoted)\b/.test(blob)) verdict = "SELL";

  const conf = verdict === "HOLD" ? 58 : 65;

  return {
    headline: `${name}: actionable update (${dt})`,
    take: `• Pulling ${rows.length} recent items (${sources}). Topic='${topic || 'general'}'.\n• Why fade: if the date/source are weak or hype outruns role.\n• Confidence: ${conf}/100`,
    verdict,
    confidence: conf,
    facts: { team, position: pos, topic, articles_considered: rows.length },
    citations: rows.map(r => ({ title: r.title, url: r.url, published_at: r.published_at, source: r.source }))
  };
}

// ----- Router factory
export function createRagRouter() {
  const router = express.Router();

  // admin: ingest
  router.post("/admin/rag/ingest", async (req, res) => {
    try {
      await ingestAll();
      res.json({ status: "ok" });
    } catch (e) {
      console.error(e);
      res.status(500).json({ error: e.message });
    }
  });

  // players search by name (against our local table)
  router.get("/api/rag/players/search", (req, res) => {
    const name = String(req.query.name || "").trim();
    if (!name) return res.status(400).json({ error: "name required" });
    const key = `%${norm(name)}%`;
    const rows = db.prepare("SELECT player_id, name, team, position FROM players WHERE name_key LIKE ? LIMIT 10").all(key);
    res.json({ results: rows });
  });

  // generate take
  router.get("/api/rag/take", (req, res) => {
    const playerId = String(req.query.player_id || "").trim();
    const topic = String(req.query.topic || "").trim();

    if (!playerId) return res.status(400).json({ error: "player_id required" });
    const p = db.prepare("SELECT player_id, name, team, position FROM players WHERE player_id=?").get(playerId);
    if (!p) return res.status(404).json({ error: "unknown player_id" });

    const rows = retrieveArticles({ playerId, topic });
    const payload = synthTake(p, topic, rows);

    // guardrail: ensure at least RAG_MIN_SOURCES citations if topic provided
    if ((payload.citations?.length || 0) < RAG_MIN_SOURCES && topic) {
      const more = retrieveArticles({ playerId: "", topic });
      const seen = new Set(payload.citations.map(c => c.url));
      for (const r of more) {
        if (!seen.has(r.url)) {
          payload.citations.push({ title: r.title, url: r.url, published_at: r.published_at, source: r.source });
          seen.add(r.url);
        }
        if (payload.citations.length >= RAG_MIN_SOURCES) break;
      }
    }

    res.json(payload);
  });

  return router;
}

CJS project? Change imports to const express = require('express'), etc., swap node-fetch to node-fetch-commonjs, and replace the crypto dynamic import with a top‑level const { createHash } = require('crypto').

⸻

3) Register router in your server

In your main server.js / app.js:

import express from "express";
import { createRagRouter } from "./rag.router.js";

const app = express();
// ... your other middlewares & routes

app.use(createRagRouter()); // mounts /admin/rag/* and /api/rag/*

const PORT = process.env.PORT || 5000;
app.listen(PORT, () => console.log(`OTC API on :${PORT}`));


⸻

4) First run (boot + ingest)

# start your existing server as usual
curl -X POST http://localhost:5000/admin/rag/ingest
curl "http://localhost:5000/api/rag/players/search?name=Josh%20Downs"
# take:
curl "http://localhost:5000/api/rag/take?player_id=<ID>&topic=qb%20change"


⸻

5) Frontend: same card works

You can keep using the RagTakeCard I gave you. It hits /api/rag/take exactly the same way—no UI changes.

⸻

Why this won’t collide with Tiber
	•	It reuses your running Node server—no flaky Replit background process.
	•	It doesn’t touch your Sleeper sync logic; it builds its own tiny players table (or you can JOIN to yours later).
	•	Endpoints are identical to the Python sidecar, so you can swap back to Python later with no FE changes.

If you want, I’ll also hand you a CJS variant of rag.router.js (for older Node setups) or wire this to your existing player table instead of duplicating a minimal one.