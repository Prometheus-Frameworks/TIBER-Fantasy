ðŸ“¦ Tiber Handoff â€” Generate & Load 2024 Wk 1â€“17 in Batches (No external CSV downloads)
Goal

Produce and load:

defense_dvp_2024.csv (â‰ˆ2,176 rows)

defense_context_2024.csv (544 rows)

schedule_2024.csv (â‰ˆ272 rows)

â€¦without pulling giant public CSVs. Use nfl_data_py and batch by week to avoid memory/timeouts on Replit.

1) Make a persistent folder
mkdir -p data

2) Schedule (small, one shot)

Use nfl_data_py (no need for external sources):

# scripts/ingest_schedule.py (quick version)
import nfl_data_py as nfl
import pandas as pd
import argparse

if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument("--season", type=int, required=True)
    p.add_argument("--weeks", type=str, default="1-17")  # supports "1-17" or "1,2,3"
    args = p.parse_args()

    # weeks parsing
    weeks = []
    if "-" in args.weeks:
        a,b = args.weeks.split("-")
        weeks = list(range(int(a), int(b)+1))
    else:
        weeks = [int(w) for w in args.weeks.split(",")]

    sched = nfl.import_schedules([args.season])
    sched = sched[(sched.season==args.season) & (sched.week.isin(weeks)) & (sched.season_type=="REG")]
    out = sched[["season","week","home_team","away_team"]].rename(columns={"home_team":"home","away_team":"away"})
    out.to_csv(f"data/schedule_{args.season}.csv", index=False)
    print(f"OK: data/schedule_{args.season}.csv -> {len(out)} rows")


Run:

python scripts/ingest_schedule.py --season=2024 --weeks=1-17

3) DVP (fantasy points allowed) â€” batch by week

Update your DVP script to iterate weeks to keep memory low and append:

# scripts/ingest_dvp.py (batched)
import nfl_data_py as nfl
import pandas as pd
import argparse
from pathlib import Path

def write_header(path, df):
    df.to_csv(path, index=False, mode="w")

def append_rows(path, df):
    df.to_csv(path, index=False, mode="a", header=False)

if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument("--year", type=int, required=True)
    p.add_argument("--ppr", action="store_true")
    p.add_argument("--weeks", type=str, default="1-17")
    args = p.parse_args()

    # parse weeks
    if "-" in args.weeks:
        a,b = args.weeks.split("-")
        weeks = list(range(int(a), int(b)+1))
    else:
        weeks = [int(w) for w in args.weeks.split(",")]

    positions = ["QB","RB","WR","TE"]
    fp_col = "fantasy_points_ppr" if args.ppr else "fantasy_points"
    out_path = Path(f"data/defense_dvp_{args.year}.csv")

    wrote_header = False
    for wk in weeks:
        weekly = nfl.import_weekly_data([args.year])
        weekly = weekly[(weekly["season"]==args.year) & (weekly["week"]==wk) & (weekly["position"].isin(positions))]
        # aggregate fantasy points scored *against* each defense
        dvp = weekly.groupby(["season","week","opponent_team","position"])[fp_col].sum().reset_index()
        dvp = dvp.rename(columns={"opponent_team":"def_team", fp_col:"fp_allowed"})

        if not wrote_header:
            write_header(out_path, dvp)
            wrote_header = True
        else:
            append_rows(out_path, dvp)
        print(f"Wrote week {wk}: {len(dvp)} rows")

    print(f"OK: {out_path} generated")


Run:

python scripts/ingest_dvp.py --year=2024 --ppr --weeks=1-17

4) Context (EPA/Pace/RZ/Venue) â€” batch by week, append

Modify Grokâ€™s script to pull pbp per week and append (no giant season pull):

# scripts/sos_context_ingest.py (batched)
import nfl_data_py as nfl
import pandas as pd
import argparse
from pathlib import Path

TEAM_HFA = { # same as Grokâ€™s map
    'ARI':1.50,'ATL':2.64,'BAL':3.06,'BUF':5.32,'CAR':1.21,'CHI':2.81,'CIN':1.83,'CLE':2.67,'DAL':3.67,'DEN':5.26,
    'DET':6.25,'GB':2.68,'HOU':1.47,'IND':0.98,'JAX':2.88,'KC':-4.09,'LAC':1.36,'LAR':0.82,'LV':2.38,'MIA':5.21,
    'MIN':0.75,'NE':1.28,'NO':-0.75,'NYG':-0.30,'NYJ':4.64,'PHI':1.71,'PIT':0.66,'SEA':0.28,'SF':2.35,'TB':1.77,
    'TEN':-0.34,'WAS':0.99
}
SCALE = 208.0
def compute_adj(hfa):
    home_adj = -(hfa / SCALE); away_adj = -home_adj; return home_adj, away_adj

def process_week(pbp_wk, season, week):
    epa_df = pbp_wk[pbp_wk['epa'].notna()].groupby(['defteam'])['epa'].mean().reset_index(name='epa_per_play_allowed')
    plays_df = pbp_wk[pbp_wk['epa'].notna()].groupby(['defteam']).size().reset_index(name='plays_allowed_per_game')
    pbp_wk['red_zone'] = pbp_wk['yardline_100'] <= 20
    drive_groups = ['game_id','drive','defteam','posteam']
    rz_trip = pbp_wk.groupby(drive_groups)['red_zone'].max().reset_index(name='is_rz_trip')
    rz_trip = rz_trip[rz_trip['is_rz_trip']==1]
    td_in_drive = pbp_wk.groupby(drive_groups)['touchdown'].max().reset_index(name='had_td')
    rz_trip = rz_trip.merge(td_in_drive, on=drive_groups)
    rz_df = rz_trip.groupby(['defteam'])['had_td'].mean().reset_index(name='rz_td_rate_allowed')
    df = pd.merge(epa_df, plays_df, on=['defteam'], how='outer')
    df = pd.merge(df, rz_df, on=['defteam'], how='outer')
    df['season'] = season; df['week'] = week
    df = df.rename(columns={'defteam':'def_team'})
    all_teams = pd.DataFrame({'def_team': list(TEAM_HFA.keys())})
    df = all_teams.merge(df, on='def_team', how='left')
    df['epa_per_play_allowed'] = df['epa_per_play_allowed'].fillna(0)
    df['plays_allowed_per_game'] = df['plays_allowed_per_game'].fillna(60)
    df['rz_td_rate_allowed'] = df['rz_td_rate_allowed'].fillna(0.55)
    df['home_def_adj'] = df['def_team'].apply(lambda t: compute_adj(TEAM_HFA.get(t,0))[0])
    df['away_def_adj'] = df['def_team'].apply(lambda t: compute_adj(TEAM_HFA.get(t,0))[1])
    return df[['season','week','def_team','epa_per_play_allowed','plays_allowed_per_game','rz_td_rate_allowed','home_def_adj','away_def_adj']]

if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument("--season", type=int, required=True)
    p.add_argument("--weeks", type=str, default="1-17")
    args = p.parse_args()
    if "-" in args.weeks:
        a,b = args.weeks.split("-"); weeks = list(range(int(a), int(b)+1))
    else:
        weeks = [int(w) for w in args.weeks.split(",")]

    out = Path(f"data/defense_context_{args.season}.csv")
    wrote_header = False
    for wk in weeks:
        # Pull only this week to keep memory low
        pbp = nfl.import_pbp_data([args.season])
        pbp = pbp[(pbp['season']==args.season) & (pbp['season_type']=="REG") & (pbp['week']==wk)]
        df = process_week(pbp, args.season, wk)
        mode = "w" if not wrote_header else "a"
        df.to_csv(out, index=False, mode=mode, header=(not wrote_header))
        wrote_header = True
        print(f"Wrote week {wk}: {len(df)} rows")

    print(f"OK: {out} generated")


Run:

python scripts/sos_context_ingest.py --season=2024 --weeks=1-17


If Replit still struggles, do smaller batches: --weeks=1-6, then 7-12, then 13-17 (the script appends).

5) Load to Postgres (fresh replace for 2024)
BEGIN;
DELETE FROM defense_dvp     WHERE season=2024;
DELETE FROM defense_context WHERE season=2024;
DELETE FROM schedule        WHERE season=2024;

\COPY defense_dvp(season,week,def_team,position,fp_allowed)
FROM 'data/defense_dvp_2024.csv' CSV HEADER;

\COPY defense_context(season,week,def_team,epa_per_play_allowed,plays_allowed_per_game,rz_td_rate_allowed,home_def_adj,away_def_adj)
FROM 'data/defense_context_2024.csv' CSV HEADER;

\COPY schedule(season,week,home,away)
FROM 'data/schedule_2024.csv' CSV HEADER;
COMMIT;


Verify:

SELECT COUNT(*) FROM defense_dvp WHERE season=2024;            -- ~2176
SELECT COUNT(*) FROM defense_context WHERE season=2024;        -- 544
SELECT COUNT(*) FROM schedule WHERE season=2024;               -- ~272
SELECT MIN(week),MAX(week) FROM defense_context WHERE season=2024; -- 1,17

6) Eye-test

/sos defaults to Season=2024; weeks 1â€“17 are populated in FPA and Contextual.

Debug shows FPA | EPA | Pace | RZ | Venue.

TL;DR for Tiber to read:

Donâ€™t fight giant public CSVs.

Generate in batches with nfl_data_py, append to CSVs, then \COPY.

If RAM issues, do weeks in smaller chunks (1â€“6, 7â€“12, 13â€“17).

If Tiber hits a specific error (timeout, memory, module import), paste it here and Iâ€™ll give the exact fix.