# EPA Sanity Check System - Technical Analysis & Recommendations

I‚Äôll provide a comprehensive analysis of your system and actionable solutions to improve calibration accuracy.

## üéØ Executive Assessment

**Good News:** Your architecture is solid, and 75% directional accuracy with hand-tuned weights is actually respectable for a reverse-engineering effort.

**Core Issue:** You‚Äôre trying to reverse-engineer Baldwin‚Äôs proprietary methodology from published outputs only. This is fundamentally challenging because:

- You don‚Äôt know his exact metric definitions (e.g., what qualifies as a ‚Äúdrop‚Äù?)
- You don‚Äôt know if he uses interaction effects or non-linear adjustments
- You don‚Äôt know his data cleaning rules or edge case handling
- 34 data points is barely sufficient for calibrating 4+ parameters

-----

## üîç Diagnostic Framework

### 1. **Immediate Diagnostics to Run**

Create a diagnostic report that breaks down each component:

```typescript
// Add to epaSanityCheck.ts
interface DiagnosticReport {
  qbName: string;
  
  // Raw metrics
  rawEPA: number;
  dropRate: number;
  pressureRate: number;
  yacDelta: number;
  defEPA: number;
  
  // Deviations from league average
  dropDeviation: number;
  pressureDeviation: number;
  yacDeviation: number;
  defDeviation: number;
  
  // Individual adjustments
  dropAdj: number;      // dropDeviation √ó 4.5
  pressureAdj: number;  // pressureDeviation √ó 1.8
  yacAdj: number;       // yacDeviation √ó -0.75
  defAdj: number;       // defDeviation √ó -1.0
  
  // Final results
  totalAdjustment: number;
  tiberAdjustedEPA: number;
  baldwinAdjustedEPA: number;
  difference: number;
  
  // Implied Baldwin adjustment (reverse engineer)
  baldwinImpliedAdjustment: number; // baldwinAdj - rawEPA
}
```

**Why this matters:** This lets you see which specific factors are causing divergence for each QB.

-----

### 2. **Critical Validation Steps**

#### A. Verify League Averages Match Baldwin‚Äôs Dataset

```python
# In epaProcessor.py - add validation output
print(f"Sample size: {total_qbs} QBs, {total_plays} plays")
print(f"League avg drop rate: {league_avg_drop_rate:.4f}")
print(f"League avg pressure rate: {league_avg_pressure_rate:.4f}")
print(f"League avg YAC delta: {league_avg_yac_delta:.4f}")
print(f"League avg def EPA: {league_avg_def_epa:.4f}")

# Compare to Baldwin's population
# If you're including all QBs but Baldwin only published 34 starters,
# your league averages will be polluted by low-volume backups
```

**Red flag check:** Are you calculating league averages from:

- Only Baldwin‚Äôs 34 QBs? ‚úÖ Correct
- All QBs in NFLfastR? ‚ùå Wrong - includes backups/injured players

#### B. Validate Context Metric Calculations

For each QB, manually verify one game against [NFL.com](http://NFL.com) or PFF data:

- Count drops yourself from game film
- Verify pressure events match official stats
- Check if your YAC calculation matches broadcast stats

**Common NFLfastR pitfalls:**

- Dropped passes: Does Baldwin count throwaways/spikes as drops?
- Pressure: Are you using `qb_hit`, `qb_hurry`, both, or a combined metric?
- YAC: Are you calculating per-completion or per-attempt?

-----

## üéõÔ∏è Weight Calibration Strategy

### **Recommended Approach: Multi-Stage Calibration**

#### Stage 1: Linear Regression Baseline

```python
# calibration_script.py
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load your data for Baldwin's 34 QBs
X = np.array([
    [drop_deviation_1, pressure_deviation_1, yac_deviation_1, def_deviation_1],
    [drop_deviation_2, pressure_deviation_2, yac_deviation_2, def_deviation_2],
    # ... for all 34 QBs
])

# Target = Baldwin's implied adjustments (Baldwin Adj EPA - Raw EPA)
y = np.array([
    baldwin_adj_1 - raw_epa_1,
    baldwin_adj_2 - raw_epa_2,
    # ... for all 34 QBs
])

# Fit linear regression
model = LinearRegression(fit_intercept=True)  # intercept handles systematic bias
model.fit(X, y)

print("Optimized weights:")
print(f"Drop: {model.coef_[0]:.2f} (current: 4.5)")
print(f"Pressure: {model.coef_[1]:.2f} (current: 1.8)")
print(f"YAC: {model.coef_[2]:.2f} (current: -0.75)")
print(f"Defense: {model.coef_[3]:.2f} (current: -1.0)")
print(f"Intercept: {model.intercept_:.4f} (systematic bias)")

print(f"\nR¬≤ score: {r2_score(y, model.predict(X)):.3f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y, model.predict(X))):.4f}")
```

**Expected outcome:** This will give you data-driven weights and reveal if there‚Äôs a systematic bias (non-zero intercept).

#### Stage 2: Constrained Optimization (if weights seem unrealistic)

```python
from scipy.optimize import minimize

def objective(weights):
    """Minimize RMSE against Baldwin's adjustments"""
    predictions = X @ weights
    return np.sqrt(np.mean((y - predictions) ** 2))

# Initial guess (your current weights)
x0 = [4.5, 1.8, -0.75, -1.0]

# Constraints (optional - add if you want theoretically plausible ranges)
bounds = [
    (0, 10),     # Drop: positive, reasonable magnitude
    (0, 5),      # Pressure: positive, smaller than drop
    (-3, 0),     # YAC: negative (good receivers help QB)
    (-3, 0),     # Defense: negative (tough defenses hurt QB)
]

result = minimize(objective, x0, bounds=bounds, method='L-BFGS-B')
print("Optimized weights:", result.x)
print("Final RMSE:", result.fun)
```

#### Stage 3: Check for Non-Linear Effects

```python
# Test if adding quadratic terms improves fit
X_poly = np.column_stack([
    X,
    X[:, 0]**2,  # Drop¬≤ (diminishing returns?)
    X[:, 1]**2,  # Pressure¬≤ (extreme pressure matters more?)
    X[:, 0] * X[:, 1],  # Drop √ó Pressure interaction?
])

model_poly = LinearRegression()
model_poly.fit(X_poly, y)

print(f"Linear R¬≤: {r2_score(y, model.predict(X)):.3f}")
print(f"Polynomial R¬≤: {r2_score(y, model_poly.predict(X_poly)):.3f}")

# If polynomial R¬≤ is significantly higher, Baldwin may use non-linear adjustments
```

-----

## üö® Likely Root Causes of Your -0.256 to +0.033 Range

### **Hypothesis 1: Data Timeframe Mismatch** (Most Likely)

**Issue:** Baldwin published data on October 15, 2025 ‚Äúthrough Week 6.‚Äù You‚Äôre pulling 2025 data, but:

- Week 6 games occurred Oct 13-14, 2025
- If you‚Äôre pulling data on Oct 17, you might include Week 7 games
- Or you might be missing Week 6 if your data updates lag

**Fix:**

```python
# In epaProcessor.py, explicitly filter to Week 6
pbp = pbp[(pbp['season'] == 2025) & (pbp['week'] <= 6)]
```

### **Hypothesis 2: League Average Population Mismatch** (Very Likely)

**Issue:** You calculate league averages from all QBs in your dataset, but Baldwin only published 34 starting QBs. If you‚Äôre including backups:

- Your league averages are lower (backups are worse)
- This inflates adjustments for starters

**Fix:**

```python
# Only calculate league averages from Baldwin's 34 QBs
baldwin_qbs = ['J. Allen', 'J. Hurts', 'P. Mahomes', ...]  # List of 34

filtered_pbp = pbp[pbp['passer'].isin(baldwin_qbs)]
league_avg_drop_rate = filtered_pbp['drop_rate'].mean()
# ... etc for other metrics
```

### **Hypothesis 3: Metric Definition Mismatch** (Possible)

**Issue:** Your definition of ‚Äúdrops‚Äù or ‚Äúpressure‚Äù may differ from Baldwin‚Äôs.

**Test:**

```python
# In epaProcessor.py, output detailed counts for spot-check
for qb in ['J. Allen', 'J. Flacco']:  # Test cases
    qb_plays = pbp[pbp['passer'] == qb]
    print(f"\n{qb} diagnostics:")
    print(f"Total dropbacks: {len(qb_plays)}")
    print(f"Drops: {qb_plays['pass_dropped'].sum()}")
    print(f"Pressures: {qb_plays['qb_pressured'].sum()}")
    print(f"Avg YAC: {qb_plays['yards_after_catch'].mean():.2f}")
```

Then manually verify these numbers make sense for Week 1-6 games.

-----

## üìä Answers to Your Specific Questions

### **1. Are our weights (4.5, 1.8, -0.75, -1.0) theoretically sound?**

**Partially.** The directional logic is correct:

- ‚úÖ Drops (positive): Bad luck, boost QB
- ‚úÖ Pressure (positive): Bad O-line, boost QB
- ‚úÖ YAC (negative): Good receivers, penalize QB
- ‚úÖ Defense (negative): Weak defenses, penalize QB

**But the magnitudes are guesses.** You need data-driven calibration. The fact that you‚Äôre getting 75% directional accuracy suggests the signs are right but the scales are wrong.

### **2. Should we use linear or non-linear adjustments?**

**Start linear, test non-linear.**

Linear is preferable because:

- Easier to interpret and explain
- Less prone to overfitting with only 34 data points
- Baldwin likely uses linear for transparency

But test if adding quadratic/interaction terms significantly improves R¬≤. If polynomial R¬≤ > 0.85 and linear R¬≤ < 0.70, Baldwin might use non-linear adjustments.

### **3. Are we missing key context factors Baldwin uses?**

**Possibly.** Baldwin mentions these factors publicly, but may include others:

- **Air yards:** Deep throws are riskier (Baldwin has tweeted about this)
- **Third-down conversion rate:** Context-specific efficiency
- **Time of possession:** Clock management impact
- **Red zone attempts:** TD opportunity quality

**Recommendation:** Run correlation analysis between your current residuals and these potential missing factors. If any show strong correlation (r > 0.4), Baldwin might be using them.

### **4. Should we use regression, gradient descent, or manual tuning?**

**Use regression first (Stage 1), then optimization (Stage 2) if needed.**

- Regression: Fast, interpretable, gives you the ‚Äúmathematically optimal‚Äù linear weights
- Gradient descent/optimization: Useful if you add constraints or non-linear terms
- Manual tuning: ‚ùå Don‚Äôt do this - you‚Äôll never converge with 4+ parameters

### **5. Is 34 QBs enough data for calibration?**

**Barely.** You have 34 observations and 4 parameters = 8.5 samples per parameter.

**Rule of thumb:** Need 10-15 samples per parameter for robust linear regression.

**Mitigation strategies:**

- Use regularization (Ridge/Lasso regression) to prevent overfitting
- Validate on future weeks (when Baldwin publishes Week 7+ data)
- Consider reducing parameters if some show weak effects

### **6. How do we avoid overfitting?**

```python
from sklearn.linear_model import RidgeCV

# Use cross-validated Ridge regression
alphas = [0.001, 0.01, 0.1, 1.0, 10.0]
model_ridge = RidgeCV(alphas=alphas, cv=5)  # 5-fold CV
model_ridge.fit(X, y)

print(f"Best alpha: {model_ridge.alpha_}")
print(f"Ridge weights: {model_ridge.coef_}")
```

This penalizes extreme weights and improves generalization.

-----

## üèà RB Extension Methodology

### **Your factors are solid:**

1. ‚úÖ **Box count rate** - Directly affects RB difficulty
1. ‚úÖ **Yards before contact (YBC)** - O-line quality proxy
1. ‚úÖ **Broken tackle rate** - Individual skill
1. ‚úÖ **Target share** - Receiving usage
1. ‚úÖ **Goal line conversion** - Opportunity quality
1. ‚úÖ **Defense faced** - Strength of schedule

### **Suggested approach:**

Since you have no reference data, use **theoretical priors** for weights:

```typescript
// Suggested RB adjustment weights (educated guesses)
const RB_WEIGHTS = {
  boxCountAdj: 2.0,      // 8+ box significantly impacts EPA
  ybcAdj: 1.5,           // O-line quality is major factor
  brokenTackleAdj: -1.0, // Skill penalty (negative = RB made it easier)
  targetShareAdj: -0.5,  // Receiving usage helps EPA
  goalLineAdj: -0.8,     // TD opportunities inflate EPA
  defenseAdj: -1.0,      // Same as QB methodology
};
```

**Validation approach:**

1. Calculate adjusted EPA for all RBs
1. Compare to expert rankings (PFF, Football Outsiders)
1. Check if adjustments make intuitive sense (e.g., Derrick Henry should get major boost for high box counts)
1. **Consider publishing as original research** - there‚Äôs genuine value here!

-----

## üîß Implementation Roadmap

### **Week 1: Diagnostics & Data Validation**

```typescript
// Task 1: Add diagnostic logging
export async function runDiagnostics() {
  const qbs = await getQBsWithContext();
  
  for (const qb of qbs) {
    const diagnostic = {
      qbName: qb.name,
      // ... (use DiagnosticReport interface above)
    };
    
    console.log(diagnostic);
    
    // Flag suspicious cases
    if (Math.abs(diagnostic.difference) > 0.10) {
      console.warn(`‚ö†Ô∏è Large divergence for ${qb.name}`);
    }
  }
}
```

```python
# Task 2: Validate Python data extraction
# In epaProcessor.py, add verbose logging mode
if VERBOSE:
    print(f"\n=== Data Validation ===")
    print(f"Week range: {pbp['week'].min()} to {pbp['week'].max()}")
    print(f"Total QBs: {pbp['passer'].nunique()}")
    print(f"Total plays: {len(pbp)}")
    print(f"\nSample QB (Josh Allen):")
    allen_plays = pbp[pbp['passer'] == 'J.Allen']
    print(f"  Dropbacks: {len(allen_plays)}")
    print(f"  Drops: {allen_plays['pass_dropped'].sum()}")
    print(f"  Pressures: {allen_plays['qb_pressured'].sum()}")
```

**Deliverable:** Diagnostic report identifying which QBs have largest errors and which factors contribute most.

### **Week 2: Weight Calibration**

```python
# Task 3: Run calibration script
# Create calibration_pipeline.py

def calibrate_weights():
    # Load Baldwin's 34 QBs data
    X, y = load_calibration_data()
    
    # Stage 1: OLS regression
    model_ols = LinearRegression()
    model_ols.fit(X, y)
    ols_rmse = compute_rmse(model_ols, X, y)
    
    # Stage 2: Ridge regression (prevent overfitting)
    model_ridge = RidgeCV(alphas=[0.001, 0.01, 0.1, 1.0, 10.0], cv=5)
    model_ridge.fit(X, y)
    ridge_rmse = compute_rmse(model_ridge, X, y)
    
    # Stage 3: Test non-linear
    X_poly = add_polynomial_features(X)
    model_poly = LinearRegression()
    model_poly.fit(X_poly, y)
    poly_rmse = compute_rmse(model_poly, X_poly, y)
    
    # Select best model
    best_model = min([
        (ols_rmse, 'OLS', model_ols),
        (ridge_rmse, 'Ridge', model_ridge),
        (poly_rmse, 'Polynomial', model_poly)
    ], key=lambda x: x[0])
    
    print(f"Best model: {best_model[1]}")
    print(f"RMSE: {best_model[0]:.4f}")
    print(f"Weights: {best_model[2].coef_}")
    
    return best_model[2]

if __name__ == '__main__':
    optimal_model = calibrate_weights()
    save_weights_to_config(optimal_model)
```

**Deliverable:** Optimized weights with <0.05 RMSE against Baldwin‚Äôs data.

### **Week 3: System Validation & RB Methodology**

```typescript
// Task 4: Update epaSanityCheck.ts with new weights
const CALIBRATED_WEIGHTS = {
  drop: 3.82,      // From calibration (example)
  pressure: 2.15,  // From calibration (example)
  yac: -0.91,      // From calibration (example)
  defense: -1.23,  // From calibration (example)
};

// Task 5: Validate against future data
// When Baldwin publishes Week 7+ data, test if your calibrated weights
// still work on new QBs (out-of-sample validation)
```

```typescript
// Task 6: Finalize RB methodology
// Document theoretical justification for RB weights
// Compare against PFF grades for validation
// Consider writing blog post about your RB adjustment methodology
```

**Deliverable:** Production system with ‚â•90% accuracy and documented RB methodology.

-----

## üéì Advanced Considerations

### **If you still can‚Äôt achieve >90% accuracy after calibration:**

1. **Data drift:** Baldwin may be using proprietary data sources beyond NFLfastR
1. **Undisclosed factors:** He may include factors he hasn‚Äôt publicly mentioned
1. **Subjective adjustments:** He may apply manual overrides for specific situations
1. **Interaction effects:** Complex interdependencies between factors

**At that point:** Consider reaching out to Baldwin directly (he‚Äôs responsive on X/Twitter) to validate your methodology or ask for methodology clarification.

### **Publishing Your Work**

If you achieve >90% accuracy, consider:

- Writing a blog post documenting your reverse-engineering process
- Publishing the RB methodology as original research (no one else has done this publicly)
- Open-sourcing the calibration pipeline for the analytics community

-----

## üìù Immediate Next Steps (This Weekend)

**Priority 1 - Data Validation (2 hours):**

```bash
# Verify Week 6 filter
python server/epaProcessor.py --verbose --week-max 6

# Check if league averages match Baldwin's population
python verify_league_averages.py --qbs-only-baldwin-34
```

**Priority 2 - Diagnostic Report (3 hours):**

```bash
# Run diagnostics to see per-QB breakdown
npm run diagnostics:epa
```

**Priority 3 - Weight Calibration (4 hours):**

```bash
# Create and run calibration script
python calibration_pipeline.py --output optimized_weights.json

# Update TypeScript service with new weights
npm run update:epa-weights -- --file optimized_weights.json
```

**Expected outcome by Monday:** You should see RMSE drop from ~0.10 to <0.05, and accuracy improve from 75% to 90%+.

-----

## ü§ù Final Thoughts

Your system architecture is excellent - the issue is purely calibration, which is solvable. The fact that you achieved 75% directional accuracy with hand-tuned weights suggests your fundamental approach is sound.

The most likely culprits are:

1. **Week filter mismatch** (you‚Äôre including Week 7, Baldwin isn‚Äôt)
1. **League average population** (you‚Äôre including all QBs, Baldwin used only 34 starters)
1. **Weight magnitudes** (directionally correct but scales wrong)

Run the diagnostics this weekend, then the calibration script. You should see dramatic improvement. If you‚Äôre still stuck after implementing Stage 1-2 calibration, the issue is likely a fundamental data mismatch that requires deeper investigation.

Would you like me to:

1. Draft the complete `calibration_pipeline.py` script with your specific data structure?
1. Create the diagnostic report TypeScript interface with color-coded accuracy indicators?
1. Help design a validation framework for the RB methodology?‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã